{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import lxml.html\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from stanza.server import CoreNLPClient\n",
    "from projectmir.xmldocument import XMLDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2020-08-24 07:27:58 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-08-24 07:27:58 INFO: Use device: cpu\n",
      "2020-08-24 07:27:58 INFO: Loading: tokenize\n",
      "2020-08-24 07:27:58 INFO: Done loading processors!\n",
      "0it [00:39, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data.\n",
      "processing data...\n",
      "preprocessed document\n",
      "extract identifiers...\n",
      "Number of math components is 50\n",
      "POS tagging...\n",
      "Starting server with command: java -Xmx16G -cp /Users/kato/GoogleDrive/project-mir/data/resources/stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 600000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-58446ed220a64076.props -preload tokenize,ssplit,pos\n",
      "extract candidate definition...\n",
      "Starting server with command: java -Xmx16G -cp /Users/kato/GoogleDrive/project-mir/data/resources/stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-0e9af834d8fd4b2b.props -preload tokenize,ssplit,pos,lemma,ner,depparse\n",
      "elapsed time: 39.66465 seconds ---\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "p_path = './data/test_latexml/'\n",
    "p = Path(p_path)\n",
    "x = list(p.glob('*.html'))\n",
    "documents_IDs = [x_.name[:-5] for x_ in x]\n",
    "\n",
    "docs_list = [[]]*len(documents_IDs)\n",
    "for i, documents_ID in tqdm(enumerate(documents_IDs)):\n",
    "    document_path = p_path + documents_ID + '.html'\n",
    "    start_time = time.time()\n",
    "    docs_list[i] = XMLDocument(document_path)\n",
    "    docs_list[i].pattern_based_extract_description()\n",
    "    if i==0:\n",
    "        break\n",
    "save_filename = 'docs_list' + time.strftime('_%d%b%Y_%H%M%S') + '.pkl'\n",
    "with open(save_filename, 'wb') as f:\n",
    "    pickle.dump(docs_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "load_filename = 'docs_list.pkl'\n",
    "with open(load_filename, 'rb') as f:\n",
    "    docs_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc0 = docs_list[0]\n",
    "print(doc0.path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 数式がextract descriptionの対象になると，エラーが出る．\n",
    "doc0.pattern_based_extract_description()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "description_candidate = 'aaa'\n",
    "a = description_candidate[-6:-4]\n",
    "\n",
    "if not a:\n",
    "    print(type(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "description_candidate = 'an inlet temperature MATH0001'\n",
    "identifier = 'MATH0001'\n",
    "description_candidate[-8:-4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "description_candidate[:-8].rstrip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "description_candidate = description_candidate.replace(')', '\\)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(description_candidate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern_list = [\n",
    "    re.compile(description_candidate + r' ' + identifier),\n",
    "    re.compile(identifier + ' is ' + description_candidate),\n",
    "    re.compile('let ' + identifier + ' be ' + description_candidate),\n",
    "    re.compile(description_candidate + r' (is|are) denoted by ' + identifier),\n",
    "    re.compile(identifier + ' denotes ' + description_candidate)\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'The liquid inlet stream consists of a single component with a mass flow rate MATH0000 and an inlet temperature MATH0001 .'\n",
    "for pattern_ in pattern_list[:1]:\n",
    "    if pattern_.findall(sentence):\n",
    "        print(pattern_.pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# sentetnce segmentation using CoreNLP\n",
    "with CoreNLPClient(\n",
    "    annotators=['tokenize', 'ssplit'],#, 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref'],\n",
    "    timeout=30000,\n",
    "    memory='16G',\n",
    "    endpoint='http://localhost:9010') as client:\n",
    "    ann = client.annotate(doc0.text)\n",
    "# # get the first sentence\n",
    "# sentence = ann.sentence[0]\n",
    "# # print(*[f'id: {token.tokenBeginIndex}\\ttext:: {token.word}' for token in sentence.token], sep='\\n')\n",
    "# print(*[f'{token.word}' for token in sentence.token])\n",
    "for i, sentence in enumerate(ann.sentence):\n",
    "    # print(f'========== Sentence {i+1} tokens =======')\n",
    "    # print(*[f'id: {token.tokenBeginIndex}\\ttext:: {token.word}' for token in sentence.token], sep='\\n')\n",
    "    print(f'{i}:\\t', *[f'{token.word}' for token in sentence.token])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/stanfordnlp/stanza/issues/288\n",
    "# get noun phrases with tregex\n",
    "def noun_phrases(_client, _text, _annotators=None):\n",
    "    pattern = 'NP'\n",
    "    matches = _client.tregex(_text, pattern, annotators=_annotators)\n",
    "    return [sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence]\n",
    "    # print('\\n'.join(['\\t' + sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence]))\n",
    "with CoreNLPClient(timeout=30000, memory='16G', endpoint='http://localhost:9012') as client:\n",
    "    sentence_list = doc0.sentences[0]\n",
    "    sentence = sentence_list[0]\n",
    "    print(sentence)\n",
    "    noun_phrase_list = noun_phrases(client, sentence, _annotators='tokenize,ssplit,pos,lemma,parse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "[i.lstrip() for i in noun_phrase_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc0.sentences[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for identifier, description in zip(doc0.identifiers, doc0.candidate_noun_phrases_list):\n",
    "    print(identifier)\n",
    "    for description_ in description:\n",
    "        for description__ in description_:\n",
    "            def remove_stopwords(description):\n",
    "                description_output = description\n",
    "                # stopwords = []\n",
    "                # stopwords are tags used in tregex and common stopwords.\n",
    "                stopwords = ['\\n', '(', ')', 'NP', 'DT', 'JJ', 'NN', 'PP', 'IN', 'CC']\n",
    "                # 参考：https://qiita.com/syunyo/items/2c1ce1d765f46a5c1d72\n",
    "                \n",
    "                for stopword in stopwords:\n",
    "                    description_output = description_output.replace(stopword, '')\n",
    "                    description_output = description_output.replace('  ', ' ')\n",
    "                return description_output\n",
    "            print(remove_stopwords(description__))\n",
    "            # print(description__.replace('\\n', ''))\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open('./doc0_text.txt', 'w') as f:\n",
    "    f.write(doc0.text)\n",
    "with open('./doc0_body.txt', 'w') as f:\n",
    "    f.write(doc0.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "(doc0.candidate_noun_phrases_list[0][0] + doc0.candidate_noun_phrases_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open('./doc0_113130.txt', 'w') as f:\n",
    "    f.write('\\n'.join(doc0.sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_noun_phrases(identifiers, sentences):\n",
    "    candidate_noun_phrases_list = [[]] * len(identifiers)\n",
    "    pattern = 'NP'\n",
    "    with CoreNLPClient(\n",
    "            annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse', 'coref'],\n",
    "            timeout=600000, memory='16G') as client:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if sentence:\n",
    "                candidate_noun_phrases_list_ = []\n",
    "                for text in sentence:\n",
    "                    matches = client.tregex(text, pattern)\n",
    "                    matches = matches['sentences'][0]\n",
    "                    candidates = [matches[match]['match'].rstrip()\n",
    "                                    for match in matches]\n",
    "                    candidate_noun_phrases_list_ += candidates\n",
    "\n",
    "                print('##################')\n",
    "                print(type(candidate_noun_phrases_list_))\n",
    "                print(candidate_noun_phrases_list_)\n",
    "                print('##################')\n",
    "\n",
    "                candidate_noun_phrases_list[i] = candidate_noun_phrases_list_\n",
    "    return candidate_noun_phrases_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "xx = extract_noun_phrases(doc0.identifiers[:1], doc0.sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('\\n\\n'.join(xx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc0.tagged_sentence_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc0.candidate_noun_phrases_list[0][0]\n",
    "# -> 最初のidentifierに対するnoun candidate\n",
    "# candidate にidentifierとの距離を埋め込む．-> candidate.extract_noun_phraseとtagged_sentence_listを比較して，長さを計算する．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 最初のidentifier[0]が含まれる1つ目の文章[0]の1番目のcandidate[0]\n",
    "doc0.candidate_noun_phrases_list[0][0][0].replace('NP', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "doc0.sentences[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38264bit382pyenv8395125df6ea4f75b9cf162ac9e2c8f9",
   "language": "python",
   "display_name": "Python 3.8.2 64-bit ('3.8.2': pyenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}